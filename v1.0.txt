# 基于Java爬虫技术的微信小程序小说阅读器

## 1 引言

### 1.1 开发背景

自从1969年美国在美国国防部高级研究计划局组建了ARPANET（Advanced Research Projects Agency Network）至今，互联网已经发展过了50个年头，长达半个世纪的发展使得如今的互联网中包含着海量的数据。如何从如此大量的数据中解析到自己想要的数据，并且有效的利用所获取到的数据成为一个巨大的挑战。在检索网页数据等方面，传统的搜索引擎如：谷歌（Google）、雅虎（Yahoo）、百度（Baidu）、必应（Bing）等搜索引擎虽然十分强大，但是搜索到的页面包含着大量不需要的数据，搜索到的数据并不具有针对性。并且传统的搜索引擎多是基于关键字的搜索，对于互联网大量而丰富的数据形式如：图片、视频、音频等等并不具有高效的解析能力。因此，可以针对特定的性质的搜索数据并可以个性化定制的定向抓取网页资源的爬虫技术应运而生。
在现在的大数据、机器学习时代，计算机硬件的速度大幅度提升，而技术的进步带来的还有计算机硬件的价格不断下降，计算机算力的不断提升使得机器学习、神经网络等不再是高高再上的技术，让我们每个人都可以享受到机器学习的便利，而这些技术需要的基础之一就是大量的元数据。通过爬虫技术可以很方便的搜索到这些需要的数据。在众多的数据类型中，小说作为一种特别的类型，具有纯文本、逻辑简单、便于解析的特点，可以作为十分好的练习爬虫的资源。
微信小程序（以下简称：小程序）作为微信在2017年1月9日发布的“触手可及、即用即走”的不需安装、下载即可使用的应用，在作为小说信息的展示技术上有着极大的优势：1. 使用极为方便：只需扫码即可使用，无需安装；2. 开发方便：相对于HTML5页面，小程序提供有丰富且多样化的组件和官方的样式库（WeiUI）可以直接使用，并且小程序还提供有众多的API，可以很方便的调用，不需要考虑众多浏览器的适配问题；3. 应用范围较广：在移动终端的两大操作系统（IOS和Android）中，不需要考虑不同系统平台的适配性问题，可以在所有安装了微信的移动设备上使用。4. 分享方便：小程序是在微信这个有着10亿用户的软件平台上，在分享推广上有着无与伦比的优势。

### 1.2 爬虫现状及技术分析

#### 1.2.1 爬虫框架介绍

有很多十分优秀的开源爬虫框架，比如基于Python语言编写的Scrapy、Crawley、PySpider、Python Selenium、Cola等；基于Java语言的WebMagic、WebCollector、crawler4j、Spiderman、Jsoup、Java Selenium、HtmlUnit等；还有其他基于PHP语言的phpspider、基于C#语言的SmartSpider、基于C/C++的Spidr等
介绍下其中一些爬虫框架：1. WebMagic：是开源的Java垂直爬虫框架，目标是简化爬虫的开发流程，让开发者专注于逻辑功能的实现，支持多线程和分布式、支持爬取动态渲染的页面；2. Spiderman：是一个开源的Java的web数据抽取工具，是微内核+插件式架构，支持多线程、状态持久化、分布式、JS脚本等等特性；3. Selenium：这个是使用一个免费开源的自动化测试工具，支持在多种浏览器，可以实时看到操作的效果，对于一些需要JavaScript渲染的页面的爬取有着很好的作用。4. WebCollector：是一个无需配置，便于二次开发的Java爬虫框架（内核），另外还有WebCollector-Hadoop是WebCollector的Hadoop版本，支持分布式爬取

#### 1.2.2 爬取内容

对于爬虫来说，绝大多数情况下需要爬取的内容是在网站或者是App中。
对于网页来说，根据页面渲染的时机可以分为服务端渲染和客户端渲染。服务端渲染是指网页在服务器中生成完整的页面，基本所有的有效信息都包含在请求的HTML页面中，比如猫眼电影的网站；客户端渲染就是指页面的主要内容通过JavaScript渲染而成，真实的数据基本通过Ajax接口等形式获取，例如淘宝、微博等等。服务端渲染的页面爬取较为简单，只需要通过一些支持HTTP协议的工具即可实现爬取。而对于客户端渲染的情况，可以分为以下几种方式解决：

1. 寻找Ajax接口，可以通过Chrome/Firefox等浏览器的开发者工具查看Ajax的具体请求接口、请求接口、参数信息等内容，然后再通过HTTP请求工具模拟请求即可实现。
2. 模拟浏览器执行，这种情况主要针对于需要较为复杂的处理逻辑，可以通过一些可见即可爬的方式进行爬取，比如Selenium工具，这种直接模拟浏览器执行效率会偏低，因此可以在摸清了JavaScript的执行和加密逻辑之后，直接执行相关的JavaScript来完成逻辑处理和接口请求。

对于App来说，主要是通过针对服务器的接口进行爬取，对接口的类型可以做简单的四种划分：普通接口、加密参数接口、加密内容接口、非常规协议接口。1. 对于普通的无加密接口，通过抓包工具分析找到接口的具体请求形式，然后在通过请求工具模拟即可。常见的抓包工具有：Fiddler、Wireshark。2. 对于加密的接口一种是可以通过实时处理，实时抓取的方式，另外就是将加密逻辑破解，直接模拟其逻辑，这可能需要一些反编译的技巧。3. 对于加密内容的接口，即通过接口返回的数据是加密后的密文，需要进行专门的解密才可以获取到，可以通过可见即可爬的的工具，比如Appium（支持iOS平台和Android平台的自动化测试开源工具），也可以通过Xposed框架中的Hook框架来获取渲染结果，还可以通过反编译或者改写手机底层来实现破解。4. 对于非常规协议，即非HTTP/HTTPS协议，可以使用Wireshark来抓取所有协议的包，或者使用Tcpdump来进行TCP数据包截取。

#### 1.2.3 确定爬取的URL队列算法

爬虫系统中，如何确定爬取的URL队列是关系到爬取速度和爬取效率的重要部分，待爬取URL队列中的URL应该用怎样的队列顺序牵扯到了爬取页面的先后顺序。常用的两种爬取策略有两种：深度优先策略和广度优先策略。
深度优先策略是指网络爬虫从起始链接开始，每次抓取第一个遇到的符合规则的链接，然后继续深入，直到这个链接已经被爬取或者不符合规则，然后再跳回上级的第二个链接继续抓取。直到抓取结束。

#此处增加深度优先的示意图

广度优先策略是指网络爬虫从起始链接开始，将该页面中所有符合规则的链接放入队列中，然后依照FIFO（先入先出）的原则，访问队列中第一个链接，将其获取到的所有符合规则的链接放入队列尾部，依次访问，直到URL队列为空。

#此处增加广度优先示意图

深度优先策略主要通过递归实现，需要回溯操作，有入栈、出栈操作，运行速度较慢。广度优先策略主要通过队列实现，无需回溯操作，运行速度较快。因此在本次设计中使用广度优先策略生成URL队列。

#### 1.2.3 解析数据

##### 1.2.3.1 通过制定具体规则解析

从网络上获取信息主要有HTML、JSON、XML三种形式，针对HTML格式可以通过XPath、正则表达式、CSS选择器、Jsoup等工具进行解析；JSON格式是轻量级的数据交换格式，便于阅读编写和机器解析和生成，针对JSON可以通过FastJson、Gson、Jackson等JSON解析库解析数据；对于XML数据格式，有着DOM解析、SAX解析、JDOM解析、DOM4J解析等解析方式。

##### 1.2.3.2 智能采集爬虫数据

但是，这些解析方式需要编写对应的解析规则，如果需要爬取大量网站，一个个去配置较为繁琐，因此就需要进行智能解析。比如Safari浏览器的阅读模式就是通过智能解析得到的。因为一个网页如果从视觉上可以十分清晰的分辨出来可以通过训练机器学习的算法进行识别。通过智能方式解析网页大体需要两个步骤：1. 基于视觉上的网页分割，将网页分割几个视觉块；2. 通过机器学习训练的方式来判断各个视觉块的类型，是标题还是正文。
对于智能解析，下面分为四个方式进行划分：

1. readability算法，这个算法定义了不同区块的不同标注集合，通过权重计算来得到最可能的区块位置。
2. 疏密度计算，通过计算每个区块内的平均文本内容长度，根据疏密程度来大致区分。
3. Scrapyly自学习，是Scrapy开发的组件，指定页页面和提取结果样例，其可通过自学习提取规则，来提取其他同类型的页面。
4. 深度学习，使用深度学习来对解析位置进行有监督学习，需要大量的标注数据。
如果可以容忍一定的错误率的，可以使用智能解析来节约时间。

##### 1.2.3.3 语义化标签解析爬虫规则

语义化标签，即表明标签内容的类型。在早期时，简单的DIV+CSS就可以渲染出一个很好的web页面，而在HTML5时代，标签不仅仅满足于实现一个网页的展示，还更加的专注于网页的结构与内容。不同的语义化标签标注了网页中不同的组成部分，使用之后即使在没有CSS的情况下，HTML页面也可以很好的呈现出内容结构。语义化标签的使用给计算机识别文章内容提供了一定的便利，增强了机器可读性。
在HTML5的语义化标签中，常用到的有如下几个：1. article标签：显示一个独立的文章内容，例如一个文章、一篇新闻、一个评论等；2. section标签：定义文章中的章节、页眉、页脚等等信息；3. details标签：用于描述文档或者文档中某个部分的细节；4. aside标签：定义主要内容以外的标签，例如侧边栏等；5. figure标签：定义独立的流内容，如图像、图表、照片、代码等，这些内容与主内容有关，但是如果删除，则不对文档流产生影响。6. footer标签：定义section或者page的页脚；7. header标签：定义section或者page的页眉；8. nav标签：定义导航链接；9. summary标签：定义details元素的标题。10. mark标签：定义带有记号的文本信息；11. time标签：定义公立的时间（24小时制）或者日期，该元素可以以机器可读的方式，对日期和时间进行编码，搜索引擎也可以生成更加智能的搜索结果

#### 1.2.5 数据持久化

既需要将爬取到的数据通过一定的方式进行存储。主要有四种方式：1. 文件：如JSON、CSV、TXT、图片、视频、音频等文件格式；2. 数据库：分为关系型数据库和非关系型数据库。关系型数据库主要有MySql、Oracle、SQLServer等等，非关系型数据库主要有MongoDB、Redis等。3. 搜索引擎：比如Solr、ElasticSearch等，这些搜索引擎和一些分词器的使用，有利于我们实现爬虫信息的检索与文字匹配。常用的Java中文分词器有：word分词器、Ansj分词器、Stanford分词器、FudanNLP分词器等。

#### 1.2.6 反爬虫技术分析

许多网站并不想要爬虫访问他们的网站，因为爬虫会占用他们的带宽，但并不会给网站带来用户流量。因此出现了许许多多的反爬虫措施。常见的反爬虫措施有：非浏览器检测、封IP、使用验证码、封账号、字体反爬等等。
对于封IP的措施：1. 寻找手机站点、App站点，这些站点的反爬会相对较弱。2. 使用代理池，比如在网上获取的免费代理，一些付费代理如讯代理、阿里云代理、多贝云代理、芝麻代理等，Tor暗网代理，Socks代理，ADSL拨号主机代理如云立方等。使用代理池时要注意保证代理池中的代理实时可用，并且使不同的代理IP有间隔、无规则的使用，并且增加对无效页面的访问，减少被识别的几率。
对于验证码的措施：常见的验证码有普通的图形验证码、算术题验证码、滑动验证码、点触验证码、手机验证码、扫描二维码等。大部分的验证码都可以通过打码平台破解。

#### 1.2.7 多线程爬虫

爬虫作为网络请求密集型的应用，在每爬取一个网页时，等待网络响应请求占用了绝大部分时间，与之相比的页面解析时间几乎可以忽略不计。因此在一次爬取页面的过程中，大部分时间线程都是阻塞在请求网络资源上，因此可以通过使用多线程的设计来大幅度的提高爬取效率。

#### 1.2.8 使用爬虫的案例

1. 搜索引擎：百度、谷歌
2. 各种的比价网站：去哪儿聚合了各大航空公司票价、购物比价软件聚合了各个电商平台的商品价格信息
3. 抢票软件：通过定时的爬虫爬取12306的票价，并进行抢票
4. 新闻聚合平台：今日头条、推酷、即刻

## 2 需求分析

### 2.1 功能需求分析

系统需要实现三大功能：1. 管理员向数据库中传入网站爬虫配置；2. 爬虫通过数据库中的对应网站爬虫配置爬取整个网站中的小说链接；3. 通过微信小程序向用户展示爬取到的链接信息

#程序流程图

### 2.2 性能需求分析

系统最耗时的操作在爬虫请求网络资源时候，可以通过配置多个线程增强爬虫性能，提高系统的执行效率。系统最大的可利用硬盘容量为30G，所用到的CPU为2.50GHz的单核CPU，网络出口带宽为5Mbps，入网带宽不超过100Mbps，内存为1838MB。

### 2.3 环境需求分析

所用到的服务器为阿里云的1核2GB内存5Mbps带宽1000G流量的轻量应用服务器

### 2.4 数据需求分析

#数据DTD图

### 2.5 文档需求分析

需要对网站规则配置的各项说明

### 2.6 安全保密需求分析

需要针对所有用户输入接口进行安全检测，防止SQL注入等，针对所有的错误信息都不应该直接向用户展示。

### 2.7 可靠性需求分析

程序中可能有的错误都不应该影响程序的正常运行